{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 14:48:11.992754: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-26 14:48:12.031879: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-26 14:48:12.032666: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-26 14:48:12.678872: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the dataset\n",
    "dataset_path = '.'\n",
    "\n",
    "# Set the paths to images and annotations\n",
    "images_folder = os.path.join(dataset_path, 'Dataset')\n",
    "annotations_folder = os.path.join(dataset_path, 'Bounding_Boxes')\n",
    "\n",
    "# Set the number of classes\n",
    "num_classes = 3\n",
    "\n",
    "# Model configuration\n",
    "input_shape = (224, 224, 3)  # Adjust based on your image size\n",
    "batch_size = 32\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 224, 224, 3), 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data preprocessing and loading\n",
    "def load_data():\n",
    "    images = []\n",
    "    annotations = []\n",
    "    label_map = {}\n",
    "    label_id = 1\n",
    "\n",
    "    # return images, labels\n",
    "    for annotation_name in os.listdir(annotations_folder):\n",
    "\n",
    "        # If no .txt files, continue\n",
    "        if not annotation_name.endswith('.txt'):\n",
    "            continue\n",
    "\n",
    "        # Load image\n",
    "        image_name = annotation_name.replace('.txt', '.jpg')\n",
    "        image_path = os.path.join(images_folder, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "        image_width, image_height = image.shape[1], image.shape[0]\n",
    "        image = cv2.resize(image, (input_shape[0], input_shape[1]))\n",
    "\n",
    "        images.append(image)\n",
    "\n",
    "        # Load annotations\n",
    "        annotation_path = os.path.join(annotations_folder, annotation_name)\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            # Parse bounding box coordinates and class label\n",
    "            # Example format: class x_min y_min x_max y_max\n",
    "            lines = f.readlines()\n",
    "            sub_annotations = []\n",
    "            for line in lines:\n",
    "                line = line.strip().split(' ')\n",
    "                class_label, x_min, y_min, x_max, y_max = map(float, line)\n",
    "\n",
    "                # Convert normalized coordinates to absolute coordinates\n",
    "                x_min *= image_width\n",
    "                y_min *= image_height\n",
    "                x_max *= image_width\n",
    "                y_max *= image_height\n",
    "\n",
    "                # Rescale coordinates to input_shape\n",
    "                x_min = int(x_min * input_shape[0] / image_width)\n",
    "                y_min = int(y_min * input_shape[1] / image_height)\n",
    "                x_max = int(x_max * input_shape[0] / image_width)\n",
    "                y_max = int(y_max * input_shape[1] / image_height)\n",
    "\n",
    "                # Add class label to label map if not already present\n",
    "                if class_label not in label_map:\n",
    "                    label_map[class_label] = label_id\n",
    "                    label_id += 1\n",
    "\n",
    "                # Store annotation information\n",
    "                sub_annotations.append({\n",
    "                    'image_path': os.path.join(images_folder, f'{image_name}'),\n",
    "                    'bbox': [x_min, y_min, x_max, y_max],\n",
    "                    'class_label': label_map[class_label]\n",
    "                })\n",
    "\n",
    "            annotations.append(sub_annotations)\n",
    "\n",
    "    images = np.array(images)\n",
    "\n",
    "    return images, annotations\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "images, annotations = load_data()\n",
    "\n",
    "images.shape, len(annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\n",
    "    1: 'category_title',\n",
    "    2: 'menu_item',\n",
    "    3: 'section'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 90, 93, 56],\n",
       "  'class_label': 1},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 132, 91, 27],\n",
       "  'class_label': 1},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [51, 161, 91, 29],\n",
       "  'class_label': 1},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [51, 186, 91, 19],\n",
       "  'class_label': 1},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [50, 206, 91, 19],\n",
       "  'class_label': 1},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [167, 20, 87, 22],\n",
       "  'class_label': 1},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [167, 41, 86, 20],\n",
       "  'class_label': 1},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [50, 68, 86, 8],\n",
       "  'class_label': 2},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [48, 122, 84, 6],\n",
       "  'class_label': 2},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [49, 152, 85, 8],\n",
       "  'class_label': 2},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [50, 183, 87, 8],\n",
       "  'class_label': 2},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [51, 201, 85, 9],\n",
       "  'class_label': 2},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [165, 16, 82, 8],\n",
       "  'class_label': 2},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [166, 36, 80, 7],\n",
       "  'class_label': 2},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 74, 86, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 78, 85, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 81, 86, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 85, 86, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 88, 86, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 91, 86, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [51, 95, 87, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [51, 98, 87, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 101, 86, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 105, 87, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 108, 87, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 112, 87, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 128, 85, 4],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 132, 85, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 135, 85, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 139, 86, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [51, 142, 86, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 158, 85, 4],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 162, 86, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 165, 86, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 169, 86, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 173, 85, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [51, 189, 86, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [51, 192, 86, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 208, 83, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [52, 212, 83, 3],\n",
       "  'class_label': 3},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [166, 23, 80, 3],\n",
       "  'class_label': 2},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [166, 26, 80, 3],\n",
       "  'class_label': 2},\n",
       " {'image_path': './Dataset/image1.jpg',\n",
       "  'bbox': [168, 44, 82, 8],\n",
       "  'class_label': 2}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-challenge-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
